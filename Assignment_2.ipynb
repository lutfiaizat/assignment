{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "784eddb9cc11411b86dc1b24295cc400": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_07d07449557f4691bea7b4e7667b6d26",
              "IPY_MODEL_b4e7bcfdc4fe41569e45991c2d867913",
              "IPY_MODEL_c6be7972ff4241c2bdd851e7c13c8da4"
            ],
            "layout": "IPY_MODEL_a4da358f320a4a23b9003f5eddd4e9e9"
          }
        },
        "07d07449557f4691bea7b4e7667b6d26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e812e3f23774fa5b8dd40a2bae5d61d",
            "placeholder": "​",
            "style": "IPY_MODEL_9e25b1fa4fda4fcdb8e70e0dc3b63e46",
            "value": "100%"
          }
        },
        "b4e7bcfdc4fe41569e45991c2d867913": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32c59a681a944b9096eed33f02f5fe00",
            "max": 46830571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5af97ac94c6a47e4a3d7b0fd56b1017d",
            "value": 46830571
          }
        },
        "c6be7972ff4241c2bdd851e7c13c8da4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc791fb9f83d4199a39d856e87194494",
            "placeholder": "​",
            "style": "IPY_MODEL_f8b12f5470b24385b7607899bb66929f",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 143MB/s]"
          }
        },
        "a4da358f320a4a23b9003f5eddd4e9e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e812e3f23774fa5b8dd40a2bae5d61d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e25b1fa4fda4fcdb8e70e0dc3b63e46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32c59a681a944b9096eed33f02f5fe00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5af97ac94c6a47e4a3d7b0fd56b1017d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cc791fb9f83d4199a39d856e87194494": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8b12f5470b24385b7607899bb66929f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lutfiaizat/assignment/blob/assessment_and_assignment_2/Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeLIYWn0tm-3"
      },
      "outputs": [],
      "source": [
        "# Workflow of training dataset to defined CNN/ any CNN model\n",
        "\n",
        "# 1. Defining dataset transform\n",
        "# 2. Defining the data batch for Train and Test\n",
        "# 3. Create dictionary for class indexes\n",
        "# 4. Defining dataset class datatype\n",
        "# 5. Preparing by finalize the dataset into sets of obects\n",
        "#    Nested transform?\n",
        "# 6. Using DataLoader to load the data (for stand-by)\n",
        "# 7. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import numpy\n",
        "import random\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "a3CLgfT41GfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Transforms to the Data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "image_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}"
      ],
      "metadata": {
        "id": "Xzkd19Fv5MRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "dataset = '/content/gdrive/My Drive/fruit_dataset'"
      ],
      "metadata": {
        "id": "TpL1c-EK1u1q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1f9ab28-3a27-4135-ab62-af206ff84e34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import cv2\n",
        "import glob\n",
        "import numpy\n",
        "import random\n",
        "from pandas.core.common import flatten\n",
        "from PIL import Image\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "EXra-JJT4Gwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "####################################################\n",
        "#       Create Train, Valid and Test sets\n",
        "####################################################\n",
        "train_data_path = dataset + '/train' \n",
        "test_data_path = dataset + '/validation'\n",
        "\n",
        "train_image_paths = [] #to store image paths in list\n",
        "classes = [] #to store class values\n",
        "\n",
        "#1.\n",
        "# get all the paths from train_data_path and append image paths and class to to respective lists\n",
        "\n",
        "for data_path in glob.glob(train_data_path + '/*'):\n",
        "    classes.append(data_path.split('/')[-1]) \n",
        "    train_image_paths.append(glob.glob(data_path + '/*'))\n",
        "    \n",
        "train_image_paths = list(flatten(train_image_paths))\n",
        "random.shuffle(train_image_paths)\n",
        "\n",
        "print('train_image_path example: ', train_image_paths[0])\n",
        "print('class example: ', classes[0])\n",
        "\n",
        "#2.\n",
        "# create the test_image_paths\n",
        "test_image_paths = []\n",
        "for data_path in glob.glob(test_data_path + '/*'):\n",
        "    test_image_paths.append(glob.glob(data_path + '/*'))\n",
        "\n",
        "test_image_paths = list(flatten(test_image_paths))\n",
        "\n",
        "print(\"Train size: {}\\nTest size: {}\".format(len(train_image_paths), len(test_image_paths)))\n"
      ],
      "metadata": {
        "id": "vpERBqfJ1zDy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57ce9311-e20f-421e-fbd7-768370710acd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_image_path example:  /content/gdrive/My Drive/fruit_dataset/train/tomato/0617fbcfe096638f44fb15cc6133976c4987c61b.jpg\n",
            "class example:  watermelon\n",
            "Train size: 517\n",
            "Test size: 323\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "#      Create dictionary for class indexes\n",
        "#######################################################\n",
        "\n",
        "idx_to_class = {i:j for i, j in enumerate(classes)}\n",
        "class_to_idx = {value:key for key,value in idx_to_class.items()}\n",
        "class_to_idx"
      ],
      "metadata": {
        "id": "FdLvjTt315ao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f08cbc0b-eec7-4b16-ebc1-0d7eb753a41b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'watermelon': 0, 'tomato': 1, 'durian': 2, 'pumpkin': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "#######################################################\n",
        "#               Define Dataset Class\n",
        "#######################################################\n",
        "\n",
        "class fruitDataset(Dataset):\n",
        "    def __init__(self, image_paths, transform=False):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_filepath = self.image_paths[idx]\n",
        "#         image = cv2.imread(image_filepath)\n",
        "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = Image.open(image_filepath) # if using torchvision transforms\n",
        "        # print(image_filepath)\n",
        "        label = image_filepath.split('/')[-2]\n",
        "        # print(label)\n",
        "        label = class_to_idx[label]\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image) # if using torchvision transforms\n",
        "        \n",
        "        return image, label"
      ],
      "metadata": {
        "id": "cbGPabN42Zgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "#                  Create Dataset\n",
        "#######################################################\n",
        "\n",
        "train_dataset = fruitDataset(train_image_paths,image_transforms['train'])\n",
        "test_dataset = fruitDataset(test_image_paths,image_transforms['test'])"
      ],
      "metadata": {
        "id": "FzyDT6hf2a1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.transform"
      ],
      "metadata": {
        "id": "vG1hvmu02n97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "830766ab-2a63-4a59-fd88-5be7e480c613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Compose(\n",
              "    RandomResizedCrop(size=(256, 256), scale=(0.8, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
              "    RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)\n",
              "    RandomHorizontalFlip(p=0.5)\n",
              "    CenterCrop(size=(224, 224))\n",
              "    ToTensor()\n",
              "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('The shape of tensor for 50th image in train dataset: ',train_dataset[49][0].shape)\n",
        "print('The label for 50th image in train dataset: ',train_dataset[49][1])"
      ],
      "metadata": {
        "id": "-XlcGRId227C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98513b0a-360d-41d8-af3d-8dde35b28d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of tensor for 50th image in train dataset:  torch.Size([3, 224, 224])\n",
            "The label for 50th image in train dataset:  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "SdXYYbpe3Dwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "#                  Create Dataloader                     #\n",
        "#######################################################\n",
        "\n",
        "# Turn train and test custom Dataset's into DataLoader's\n",
        "from torch.utils.data import DataLoader\n",
        "trainloader = DataLoader(dataset=train_dataset, # use custom created train Dataset\n",
        "                                     batch_size=32, # how many samples per batch?\n",
        "                                     num_workers=0, # how many subprocesses to use for data loading? (higher = more)\n",
        "                                     shuffle=True) # shuffle the data?\n",
        "\n",
        "testloader = DataLoader(dataset=test_dataset, # use custom created test Dataset\n",
        "                                    batch_size=32, \n",
        "                                    num_workers=0, \n",
        "                                    shuffle=False) # don't usually need to shuffle testing data\n",
        "\n",
        "train_data_size = len(trainloader.dataset)\n",
        "test_data_size = len(testloader.dataset)\n",
        "\n",
        "print(train_data_size)\n",
        "print(test_data_size)"
      ],
      "metadata": {
        "id": "3Fz_-ytW3LHK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4c1a19d-14fb-4414-b195-02d4c061ff1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "521\n",
            "323\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#batch of image tensor\n",
        "next(iter(trainloader))[0].shape"
      ],
      "metadata": {
        "id": "qVSxTcVh3VrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#batch of the corresponding labels\n",
        "next(iter(trainloader))[1].shape"
      ],
      "metadata": {
        "id": "JCZghZQr3WpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################\n",
        "# MODEL IS DEFINED\n",
        "\n",
        "model = models.resnet18(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "model.fc = nn.Linear(num_ftrs, 4)\n",
        "\n",
        "# 2. LOSS AND OPTIMIZER\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# 3. move the model to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "U8LZKSJZ8WbQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "784eddb9cc11411b86dc1b24295cc400",
            "07d07449557f4691bea7b4e7667b6d26",
            "b4e7bcfdc4fe41569e45991c2d867913",
            "c6be7972ff4241c2bdd851e7c13c8da4",
            "a4da358f320a4a23b9003f5eddd4e9e9",
            "9e812e3f23774fa5b8dd40a2bae5d61d",
            "9e25b1fa4fda4fcdb8e70e0dc3b63e46",
            "32c59a681a944b9096eed33f02f5fe00",
            "5af97ac94c6a47e4a3d7b0fd56b1017d",
            "cc791fb9f83d4199a39d856e87194494",
            "f8b12f5470b24385b7607899bb66929f"
          ]
        },
        "outputId": "90d6b09d-cccd-42cd-942d-35ecea81fd5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "784eddb9cc11411b86dc1b24295cc400"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "\n",
        "model.to(device)\n",
        "summary(model, (3, 224, 224))"
      ],
      "metadata": {
        "id": "Q4NZ6obeRN75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea3e6979-3594-49df-d62e-ebef77d93141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
            "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
            "             ReLU-14           [-1, 64, 56, 56]               0\n",
            "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
            "             ReLU-17           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
            "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
            "             ReLU-21          [-1, 128, 28, 28]               0\n",
            "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
            "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
            "             ReLU-26          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
            "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
            "             ReLU-30          [-1, 128, 28, 28]               0\n",
            "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
            "             ReLU-33          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
            "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
            "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
            "             ReLU-37          [-1, 256, 14, 14]               0\n",
            "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
            "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
            "             ReLU-42          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
            "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
            "             ReLU-46          [-1, 256, 14, 14]               0\n",
            "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
            "             ReLU-49          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
            "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
            "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-53            [-1, 512, 7, 7]               0\n",
            "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
            "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
            "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-58            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
            "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-62            [-1, 512, 7, 7]               0\n",
            "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-65            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
            "           Linear-68                    [-1, 4]           2,052\n",
            "================================================================\n",
            "Total params: 11,178,564\n",
            "Trainable params: 11,178,564\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 62.79\n",
            "Params size (MB): 42.64\n",
            "Estimated Total Size (MB): 106.00\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import time # to calculate training time\n",
        "\n",
        "def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n",
        "    '''\n",
        "    Function to train and validate\n",
        "    Parameters\n",
        "        :param model: Model to train and validate\n",
        "        :param loss_criterion: Loss Criterion to minimize\n",
        "        :param optimizer: Optimizer for computing gradients\n",
        "        :param epochs: Number of epochs (default=25)\n",
        "  \n",
        "    Returns\n",
        "        model: Trained Model with best validation accuracy\n",
        "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
        "    '''\n",
        "    \n",
        "    start = time.time()\n",
        "    history = []\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
        "        \n",
        "        # Set to training mode\n",
        "        model.train()\n",
        "        \n",
        "        # Loss and Accuracy within the epoch\n",
        "        train_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "        \n",
        "        valid_loss = 0.0\n",
        "        valid_acc = 0.0\n",
        "        \n",
        "        for i, (inputs, labels) in enumerate(trainloader):\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Clean existing gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass - compute outputs on input data using the model\n",
        "            outputs = model(inputs)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = loss_criterion(outputs, labels)\n",
        "            \n",
        "            # Backpropagate the gradients\n",
        "            loss.backward()\n",
        "            \n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Compute the total loss for the batch and add it to train_loss\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            \n",
        "            # Compute the accuracy\n",
        "            ret, predictions = torch.max(outputs.data, 1)\n",
        "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "            \n",
        "            # Convert correct_counts to float and then compute the mean\n",
        "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "            \n",
        "            # Compute total accuracy in the whole batch and add to train_acc\n",
        "            train_acc += acc.item() * inputs.size(0)\n",
        "            \n",
        "            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
        "\n",
        "            \n",
        "        # Validation - No gradient tracking needed\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Set to evaluation mode\n",
        "            model.eval()\n",
        "\n",
        "            # Validation loop\n",
        "            for j, (inputs, labels) in enumerate(testloader):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Forward pass - compute outputs on input data using the model\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = loss_criterion(outputs, labels)\n",
        "\n",
        "                # Compute the total loss for the batch and add it to valid_loss\n",
        "                valid_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                # Calculate validation accuracy\n",
        "                ret, predictions = torch.max(outputs.data, 1)\n",
        "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "\n",
        "                # Convert correct_counts to float and then compute the mean\n",
        "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "                # Compute total accuracy in the whole batch and add to valid_acc\n",
        "                valid_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
        "            \n",
        "        # Find average training loss and training accuracy\n",
        "        avg_train_loss = train_loss/train_data_size \n",
        "        avg_train_acc = train_acc/train_data_size\n",
        "\n",
        "        # Find average training loss and training accuracy\n",
        "        avg_test_loss = valid_loss/test_data_size \n",
        "        avg_test_acc = valid_acc/test_data_size\n",
        "\n",
        "        history.append([avg_train_loss, avg_test_loss, avg_train_acc, avg_test_acc])\n",
        "                \n",
        "        epoch_end = time.time()\n",
        "    \n",
        "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_test_loss, avg_test_acc*100, epoch_end-epoch_start))\n",
        "        \n",
        "        # Save if the model has best accuracy till now\n",
        "        torch.save(model, 'cifar10_model_'+str(epoch)+'.pt')\n",
        "            \n",
        "    return model, history"
      ],
      "metadata": {
        "id": "1KYKjiShR2dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 4. Train the model for 10 epochs\n",
        " \n",
        "num_epochs = 10\n",
        "trained_model, history = train_and_validate(model, criterion, optimizer, num_epochs)"
      ],
      "metadata": {
        "id": "l4g6PgYfSC_j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39b625db-acdb-4531-baf2-87f1ad9f44c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\n",
            "Epoch : 000, Training: Loss: 0.8944, Accuracy: 64.1075%, \n",
            "\t\tValidation : Loss : 1.5712, Accuracy: 64.0867%, Time: 5.4672s\n",
            "Epoch: 2/10\n",
            "Epoch : 001, Training: Loss: 0.4645, Accuracy: 88.4837%, \n",
            "\t\tValidation : Loss : 1.3640, Accuracy: 82.3529%, Time: 5.4449s\n",
            "Epoch: 3/10\n",
            "Epoch : 002, Training: Loss: 0.4156, Accuracy: 88.4837%, \n",
            "\t\tValidation : Loss : 1.2666, Accuracy: 83.9009%, Time: 5.4678s\n",
            "Epoch: 4/10\n",
            "Epoch : 003, Training: Loss: 0.3211, Accuracy: 90.9789%, \n",
            "\t\tValidation : Loss : 1.1760, Accuracy: 84.5201%, Time: 5.4531s\n",
            "Epoch: 5/10\n",
            "Epoch : 004, Training: Loss: 0.2953, Accuracy: 91.9386%, \n",
            "\t\tValidation : Loss : 1.1127, Accuracy: 83.5913%, Time: 5.4217s\n",
            "Epoch: 6/10\n",
            "Epoch : 005, Training: Loss: 0.2573, Accuracy: 92.7063%, \n",
            "\t\tValidation : Loss : 1.0479, Accuracy: 83.5913%, Time: 5.5066s\n",
            "Epoch: 7/10\n",
            "Epoch : 006, Training: Loss: 0.2547, Accuracy: 92.1305%, \n",
            "\t\tValidation : Loss : 0.9592, Accuracy: 85.1393%, Time: 5.4434s\n",
            "Epoch: 8/10\n",
            "Epoch : 007, Training: Loss: 0.2244, Accuracy: 93.0902%, \n",
            "\t\tValidation : Loss : 0.9300, Accuracy: 84.5201%, Time: 5.4961s\n",
            "Epoch: 9/10\n",
            "Epoch : 008, Training: Loss: 0.2261, Accuracy: 92.1305%, \n",
            "\t\tValidation : Loss : 0.8523, Accuracy: 83.9009%, Time: 5.4876s\n",
            "Epoch: 10/10\n",
            "Epoch : 009, Training: Loss: 0.1991, Accuracy: 93.6660%, \n",
            "\t\tValidation : Loss : 0.8404, Accuracy: 83.5913%, Time: 5.4331s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Analyze the loss curve\n",
        "\n",
        "history = np.array(history)\n",
        "plt.plot(history[:,0:2])\n",
        "plt.legend(['Tr Loss', 'Val Loss'])\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(0,3)\n",
        "# plt.savefig('cifar10_loss_curve.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "7MKGzeSoilsU",
        "outputId": "a0c4c32b-6991-49dc-9a3c-838b21b37d6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXRV9bn/8feTGRKGAFFmmazKjCJKqRa0tlgHbGut1uGit7XaXm21vbV3qsNqf9XWW61Dq9xW63Sl1lqLA6J1Qm2vFZAwUwGxBBBINJAAmZ/fH3snOQkJCSQ7J8n+vNba6+zhe/Z5chbkk+8evtvcHRERia+UZBcgIiLJpSAQEYk5BYGISMwpCEREYk5BICIScwoCEZGYiywIzCzLzP5mZvlmttrMbm6iTaaZ/c7MNpjZ22Y2Iqp6RESkaVH2CMqB09x9EjAZmG1mJzdq88/Ax+4+BrgDuC3CekREpAmRBYEHSsPF9HBqfPfaHOChcP5J4HQzs6hqEhGRA6VFuXMzSwWWAmOAe9397UZNhgBbANy9ysx2A/2Bwkb7uRK4EiA7O/uEY489NsqyRUS6naVLlxa6e15T2yINAnevBiabWV/gj2Y23t1XHcZ+5gHzAKZOnepLlixp50pFRLo3M/uguW0dctWQuxcDrwKzG23aCgwDMLM0oA9Q1BE1iYhIIMqrhvLCngBm1gM4A1jXqNkC4J/C+fOBV1yj4ImIdKgoDw0NAh4KzxOkAE+4+7NmdguwxN0XAL8BHjGzDcBHwIUR1iMiIk2ILAjcfQUwpYn1P0yYLwO+HFUNItI9VFZWUlBQQFlZWbJL6fSysrIYOnQo6enprX5PpCeLRUTaQ0FBAb169WLEiBHoCvPmuTtFRUUUFBQwcuTIVr9PQ0yISKdXVlZG//79FQItMDP69+9/yD0nBYGIdAkKgdY5nO9JQSAiEnMKAhGRFhQVFTF58mQmT57MwIEDGTJkSN1yRUXFAe1fe+01zj777CRUenh0slhEpAX9+/dn+fLlANx0003k5OTwve99r257VVUVaWld99epegQiIodh7ty5XHXVVZx00kl8//vfb9V7Hn/8cSZMmMD48eO54YYbAKiurmbu3LmMHz+eCRMmcMcddwBw1113MXbsWCZOnMiFF0Z7i1XXjTARiaWbn1nNmm172nWfYwf35sZzxh3y+woKCvjLX/5Campqi223bdvGDTfcwNKlS8nNzeWzn/0sTz/9NMOGDWPr1q2sWhUMw1ZcXAzArbfeyvvvv09mZmbduqioRyAicpi+/OUvtyoEAN555x1mzpxJXl4eaWlpXHzxxSxevJhRo0axadMmrrnmGl544QV69+4NwMSJE7n44ot59NFHIz/spB6BiHQph/OXe1Sys7PbvI/c3Fzy8/NZtGgR9913H0888QQPPPAAzz33HIsXL+aZZ57hxz/+MStXrowsENQjEBHpANOmTeP111+nsLCQ6upqHn/8cT796U9TWFhITU0NX/rSl/jRj37EsmXLqKmpYcuWLcyaNYvbbruN3bt3U1pa2vKHHCb1CEREIvDyyy8zdOjQuuXf//733HrrrcyaNQt356yzzmLOnDnk5+dz+eWXU1NTA8BPfvITqqurueSSS9i9ezfuzrXXXkvfvn0jq9W62qjPejCNSPysXbuW4447LtlldBlNfV9mttTdpzbVXoeGRERiTkEgIhJzCgIRkZhTEIiIxJyCQEQk5hQEIiIxpyAQEWnBrFmzWLRoUYN1d955J1dffXWz75k5cyZNXere3PpkUhCIiLTgoosuYv78+Q3WzZ8/n4suuihJFbUvBYGISAvOP/98nnvuubqH0GzevJlt27ZxyimncPXVVzN16lTGjRvHjTfeeFj7/+ijjzjvvPOYOHEiJ598MitWrADg9ddfr3sAzpQpUygpKWH79u2ceuqpTJ48mfHjx/PGG2+0+efTEBMi0rUs/AF8uLJ99zlwApx5a7Ob+/Xrx7Rp01i4cCFz5sxh/vz5XHDBBZgZP/7xj+nXrx/V1dWcfvrprFixgokTJx7Sx994441MmTKFp59+mldeeYXLLruM5cuXc/vtt3PvvfcyY8YMSktLycrKYt68eXzuc5/jP/7jP6iurmbfvn1t/enVIxARaY3Ew0OJh4WeeOIJjj/+eKZMmcLq1atZs2bNIe/7zTff5NJLLwXgtNNOo6ioiD179jBjxgyuv/567rrrLoqLi0lLS+PEE0/kwQcf5KabbmLlypX06tWrzT+begQi0rUc5C/3KM2ZM4frrruOZcuWsW/fPk444QTef/99br/9dt555x1yc3OZO3cuZWVl7faZP/jBDzjrrLN4/vnnmTFjBosWLeLUU09l8eLFPPfcc8ydO5frr7+eyy67rE2fox6BiEgr5OTkMGvWLK644oq63sCePXvIzs6mT58+7Nixg4ULFx7Wvk855RQee+wxIHjw/YABA+jduzcbN25kwoQJ3HDDDZx44omsW7eODz74gCOPPJKvf/3rfO1rX2PZsmVt/tnUIxARaaWLLrqIL3zhC3WHiCZNmsSUKVM49thjGTZsGDNmzGjVfs466yzS09MBmD59Ovfffz9XXHEFEydOpGfPnjz00ENAcInqq6++SkpKCuPGjePMM89k/vz5/OxnPyM9PZ2cnBwefvjhNv9ckQ1DbWbDgIeBIwEH5rn7Lxq1mQn8CXg/XPWUu99ysP1qGGqR+NEw1IfmUIehjrJHUAV8192XmVkvYKmZveTujc+kvOHuZ0dYh4iIHERk5wjcfbu7LwvnS4C1wJCoPk9ERA5Ph5wsNrMRwBTg7SY2TzezfDNbaGad56nUItKpdLWnKSbL4XxPkQeBmeUAfwC+4+57Gm1eBhzl7pOAu4Gnm9nHlWa2xMyW7Nq1K9qCRaTTycrKoqioSGHQAnenqKiIrKysQ3pfpM8sNrN04Flgkbv/vBXtNwNT3b2wuTY6WSwSP5WVlRQUFLTrNfrdVVZWFkOHDq27KqlWUk4Wm5kBvwHWNhcCZjYQ2OHubmbTCHooRVHVJCJdU3p6OiNHjkx2Gd1WlFcNzQAuBVaa2fJw3b8DwwHc/T7gfOBqM6sC9gMXuvp+IiIdKrIgcPc3AWuhzT3APVHVICIiLdMQEyIiMacgEBGJOQWBiEjMKQhERGJOQSAiEnMKAhGRmFMQiIjEnIJARCTmFAQiIjGnIBARiTkFgYhIzCkIRERiTkEgIhJzCgIRkZhTEIiIxJyCQEQk5hQEIiIxpyAQEYk5BYGISMwpCEREYk5BICIScwoCEZGYUxCIiMScgkBEJOYUBCIiMacgEBGJOQWBiEjMKQhERGIusiAws2Fm9qqZrTGz1Wb27SbamJndZWYbzGyFmR0fVT0iItK0tAj3XQV8192XmVkvYKmZveTuaxLanAkcHU4nAb8KX0VEpINE1iNw9+3uviycLwHWAkMaNZsDPOyB/wP6mtmgqGoSEZEDdcg5AjMbAUwB3m60aQiwJWG5gAPDAjO70syWmNmSXbt2RVWmiEgsRR4EZpYD/AH4jrvvOZx9uPs8d5/q7lPz8vLat0ARkZiLNAjMLJ0gBB5z96eaaLIVGJawPDRcJyIiHSTKq4YM+A2w1t1/3kyzBcBl4dVDJwO73X17VDWJiMiBorxqaAZwKbDSzJaH6/4dGA7g7vcBzwOfBzYA+4DLI6xHRESaEFkQuPubgLXQxoFvRVWDiIi0THcWi4jEnIJARCTmFAQiIjGnIBARiTkFgYhIzCkIRERiTkEgIhJzCgIRkZhTEIiIxJyCQEQk5hQEIiIxpyAQEYk5BYGISMwpCEREYk5BICIScwoCEZGYUxCIiMScgkBEJOYUBCIiMacgEBGJOQWBiEjMKQhERGKuVUFgZtlmlhLOf8LMzjWz9GhLExGRjtDaHsFiIMvMhgAvApcCv42qKBER6TitDQJz933AF4FfuvuXgXHRlSUiIh2l1UFgZtOBi4HnwnWp0ZQkIiIdqbVB8B3g34A/uvtqMxsFvBpdWSIi0lFaFQTu/rq7n+vut4UnjQvd/dqDvcfMHjCznWa2qpntM81st5ktD6cfHkb9IiLSRq29auh/zay3mWUDq4A1ZvavLbztt8DsFtq84e6Tw+mW1tQiIiLtq7WHhsa6+x7gPGAhMJLgyqFmufti4KO2lSciIlFrbRCkh/cNnAcscPdKwNvh86ebWb6ZLTSzZq9CMrMrzWyJmS3ZtWtXO3ysiIjUam0Q3A9sBrKBxWZ2FLCnjZ+9DDjK3ScBdwNPN9fQ3ee5+1R3n5qXl9fGjxURkUStPVl8l7sPcffPe+ADYFZbPtjd97h7aTj/PEGvY0Bb9ikiIoeutSeL+5jZz2sPz5jZfxP0Dg6bmQ00Mwvnp4W1FLVlny2qqY509yIiXVFrDw09AJQAF4TTHuDBg73BzB4H/gocY2YFZvbPZnaVmV0VNjkfWGVm+cBdwIXu3h7nHZpWsBTunQZb3onsI0REuqK0VrYb7e5fSli+2cyWH+wN7n5RC9vvAe5p5ee3nQHVFfDgbDjtP+GT34YUDb4qItLa34T7zexTtQtmNgPYH01JERlyAnzjDTj2bPjzTfDoF6F0Z7KrEhFJutYGwVXAvWa22cw2E/wl/43IqopKj77w5d/C2XfCP/4Kv5oBG19JdlUiIknV2quG8sPLPCcCE919CnBapJVFxQymXg5ffxV69odHvgAv3QjVlcmuTEQkKQ7pIHl4yWft/QPXR1BPxzlyLHz9FTjhcnjrTnjwTPh4c7KrEhHpcG05W2rtVkWyZPSEc+4MDhft+jvcdwqs/mOyqxIR6VBtCYLoLvXsaOO+AFcthgGfgN/PhWe+DRX7kl2ViEiHOGgQmFmJme1pYioBBndQjR0jdwRc8QJ86jpY+lv4n9Ng59pkVyUiErmDBoG793L33k1Mvdy9tfcgdB2p6fCZm+CSp2BfIcybCUsehAjvcxMRSTbdUdWUMafD1X+Boz4Jz34nOFy0vzjZVYmIREJB0JycI+DiP8BnboZ1zwYnkjU8hYh0QwqCg0lJgU99B65YFFwj9cDn4M07oKYm2ZWJiLQbBUFrDJ0aDE9x3Dn1w1OU7Eh2VSIi7UJB0Fq1w1Oc84tgeIr7ZsCGl5NdlYhImykIDoUZnDAXrnwNeg4IegYv/VDDU4hIl6YgOBxHHJcwPMUv4IHZGp5CRLosBcHhShyeovC94KqiVU8luyoRkUOmIGircV+Aq96AvGPgycthwbUankJEuhQFQXvIPQouXxgMT7HsYfifWbBjTbKrEhFpFQVBe6kdnuLSp2DfR0EYLHlAw1OISKenIGhvo0+Dq98Kh6e4Dp64TMNTiEinpiCIQuLwFOufD4en+FuyqxIRaZKCICoHDE8xG974uYanEJFOR0EQtaFT4ao3Yey58PLN8OgXNDyFiHQq3e+ZAp1RVh84/0EYNQsW3gB3jIP+o2HA0TDgmODJaAOODqbMXsmuVkRiRkHQUczghH+C4SfD8v8NbkLbuQ7WPQ9eXd+u1+AgEPISA+IT0GtQsA8RkXamIOhoecfAGTfXL1dVBMNTFK6Hwr8HAVH4d8ifD+V76ttl9KoPhbrXT0C/UZCW0eE/hoh0HwqCZEvLgLxPBFMidyjdEYTCrvX1AbH5TVgxv76dpQbPW847pmFADDgaeuR26I8iIl1TZEFgZg8AZwM73X18E9sN+AXweWAfMNfdl0VVT5djBr0GBtPIUxtuKy+Fog1hD+Lv9T2JDX+G6or6dtl54TmIRgHRZ1hwVZOICNH2CH4L3AM83Mz2M4Gjw+kk4Ffhq7QkMwcGTw6mRDXVUPwB7GoUEGuehv0f17dL6wH9x8DgSTDmM8FJ7B59O/ZnEJFOI7IgcPfFZjbiIE3mAA+7uwP/Z2Z9zWyQu2+PqqZuLyU1OGfQbxQcM7vhtr1FDXsQu9bD2mfh3UeDw0vDToKjPwNjzoCBE3RiWiRGknmOYAiwJWG5IFx3QBCY2ZXAlQDDhw/vkOK6nez+kD0djppev666CrYugfdegg0vwcu3BFOvQTDm9CAURs8KLn8VkW6rS5wsdvd5wDyAqVOnahS39pKaFlzOOvxkOP2/ghvdNvw5CIU1zwS9hZS0oLcw5jNw9GfhyHHqLYh0M8kMgq3AsITloeE6SZZeR8KUi4OpugoK3glC4b0Xg7uiX745uM9hzOlBKIyaCVm9k121iLRRMoNgAfAvZjaf4CTxbp0f6ERS04LDSEdNh9N/CCUfBr2F916ENQvg3UfC3sLJcPQZwXTEWPUWRLog84jGyzezx4GZwABgB3AjkA7g7veFl4/eA8wmuHz0cndf0tJ+p06d6kuWtNhMolRdGfQW3nspmHasDNb3HhIeQjoDRn5avQWRTsTMlrr71Ca3RRUEUVEQdEJ7ttf3Fja9FtwRnZIGw6cHoTDmDDjiOPUWRJJIQQC8t6OEny1az8+/MpmczC5xjrxrqq4Mnr3w3otBOOxYFazvPbT+8tRRn9bgeiIdTEEA/GVDIZf85m1mjx/IvV89HtNfpx1jz7b63sLG16CiBFLSg3MPY8JzCwM+EdwDISKRURCE7n99Iz9ZuI5///yxXHnq6HauTFpUXQlb3g5C4b0/w87V9dssFdKygrGX0rIgNaPRcuZBttUuZx58W1pmM/vJDF4zsnX4SrqtgwVBrI6RXHnqKPILirl14TrGD+nDJ0cPSHZJ8ZKaDiM+FUxn3AK7t8LGl4NzDNXlUBVOifOJy2V7oGpXuFwWjNxaVRaMr1RVBt7Gp79l9oH+o6Df6ODu7P6jg/n+o4MB/BQS0k3FqkcAUFpexXn3vsXHeyt45ppPMbhvj3asTpKquqpRiCSERF1oJAZMwrbKfbB7CxRthI82wu6ChsGS1ac+FBoExSjo2S95P7NIK+nQUCMbdpZy3r1vMfqIHJ74xslkpun4tDRSVQ4ffxCEQtFG+GhTOL8pCAwS/t/0yA3HeGocFKM0FLh0GgqCJryw6kOuenQpF00bzk++OKEdKpPYqCoPHiZU23v4aFN9WOwuoGFI9KvvOdQFRTgwoEZ8lQ6kcwRNmD1+IFfPHM2vXtvIlGF9ueDEYS2/SQSCk8t5xwRTY5VlQUjU9STCgNj8Fqz4XcO2PfsfGA79R0PuSIWEdKjYBgHA9z57DCsLdvOff1rFsYN6MXGo/vNJG6VnwRHHBlNjlfsb9iRqexHvL4b8xxu2zeobPHkudwT0G1k/nzsiuCcjNdb/daWdxfbQUK2P9lZwzt1vAvDMNZ+iX7ae/ytJULEPPn4/CIaPN4e9iveD1+J/QE1lfduUtOApc4nhkBgYGjZcmqBzBC1YUVDM+ff9lWkj+vHQFdNITdFlgtKJ1FQHN+Z9vDkIi9qgqA2L/R81bN8jNyEgGvcmhqg3EVMKglZ44p0tfP8PK7h65mhumN1Et16ksyrbHVzh1FRQFP8Daqrq26akQd/hB/YmakNDAwV2WzpZ3AoXnDiMd7cU86vXNjJpaF9mjx+Y7JJEWierDwyaGEyNVVfBnq0Nw6E2LLa92/BZ1hBc5ZQ7AnKPCnoPvYdAn/C192DIOVLDgXRDCoIEN507ljXb9/C93+cz5ogcxhyRk+ySRNomNS34pZ57FPDpA7fvL24UEuG0PR/WLwxuuEtkqcGjTHsPbhgQiaGhsOhydGiokW3F+znn7jfJzc7g6W/N0EilEl/usO+joEdRN20Lhgapnd+ztZmwGNgwJPo0CoycI3WuooPpHMEhqh2p9HPjBvLLizVSqUiz3IPDS3u2HhgQicFRtb/h+ywFcgYeGBCJwZEzUGHRjnSO4BB9cswAfnDmsfy/59cxb/EmvvFpjVQq0iSzYKylnv1gYDN36NeFRRMBsWcr7FgTPOmucl+jfadAzwHBvnvkhlO/4Ga7HrmN1tduy9UosodBQdCMr58yivwtu7nthXVMGNKHT47RSKUih6VBWIxvuo07lBUfeOip9MMgRPYXB1dAbc8PDlc17mEkSs1oOiCaDJCE+RgHiA4NHUTtSKUf7a3gWY1UKtJ5VO4PwmH/x8F9FPs/DqZ9CfNNTY17HYkOFiBZfSAjJwiLzJz6+YxwPjNcTs+GlJSO+x4Ogc4RtMHGXaXMuectRudl87tvTCcrXVdDiHRZlWWNwqFRcDQIkuJge0s9kMbSsxMCIxsyeoWviSFSGyoJ2xqHSu1yWma79FR0jqANRuflcPuXJ3HVo0u5+ZnV/OSLTVyrLSJdQ3oWpA+C3oMO7X3VlVCxFypKg9fy0nC+tH59ecJ84+V9hVD8QcK6ktY/SMlS6wNk2tfhU9cd+s/dAgVBK8weP5BvzhzNL1/byORhffnKicOTXZKIdKTU9PAcQzsNTOkeXHZb3ihMmgqUuuDZC32Pap/Pb0RB0Erf/ewxrNy6m//602qOG9RbI5WKyOEzg/QewUResquhc57V6IRSU4xfXDiFvJxMrnpkKUWl5ckuSUSkXSgIDkG/7Azuu+QECvdWcO38d6mqbuPD0kVEOgEFwSGaMLQPPzpvPG9tKOL2F/+e7HJERNpMQXAYLpg6jK+eNJz7Xt/IC6u2J7scEZE2iTQIzGy2ma03sw1m9oMmts81s11mtjycvhZlPe3pxnPGMnlYX777RD4bdpYmuxwRkcMWWRCYWSpwL3AmMBa4yMzGNtH0d+4+OZx+HVU97S0zLZVfXXI8WempfOORJZSWV7X8JhGRTijKHsE0YIO7b3L3CmA+MCfCz+twg/r04O6vTuH9wr386+/z6Wp3aYuIQLRBMATYkrBcEK5r7EtmtsLMnjSzYRHWE4lPjh7Av515HAtXfcj9izcluxwRkUOW7JPFzwAj3H0i8BLwUFONzOxKM1tiZkt27drVoQW2xtdOGclZEwfx0xfW8daGwmSXIyJySKIMgq1A4l/4Q8N1ddy9yN1r78z6NXBCUzty93nuPtXdp+blJf8uvMbMjJ9+aSKj83K45vF32Vp8CANUiYgkWZRB8A5wtJmNNLMM4EJgQWIDM0sc+elcYG2E9UQqOzON+y49gYqqGq5+dCllldXJLklEpFUiCwJ3rwL+BVhE8Av+CXdfbWa3mNm5YbNrzWy1meUD1wJzo6qnI4zOy+G/L5jEioLd3LRgdbLLERFpFT2PIAI/W7SOe1/dyK1fnMCF0zRSqYgk38GeR5Dsk8Xd0vVnHMMpRw/gh39aTf6W4mSXIyJyUAqCCKSmGHddOIW8Xplc/ahGKhWRzk1BEJHc7AzuvzQYqfSaxzVSqYh0XgqCCI0fEoxU+peNRfzsxfXJLkdEpEkKgohdMHUYF580nPtf38TClRqpVEQ6HwVBB/hhOFLp936fz4adJckuR0SkAQVBB6gdqbRHRipXPrKUkrLKZJckIlJHQdBBBvXpwd0XHc8HRfv45mPLyN9SrNFKRaRTUBB0oOmj+3PTOWP5v01FzLn3LWbe/hq3L1rP+g91uEhEkkd3FifB7n2VLFr9Ic+s2MZbGwqpcTjmyF6cM2kQZ08czIgB2ckuUUS6mYPdWawgSLJdJeUsXLWdBcu3seSDjwGYNLQP50wazFkTBzGoT48kVygi3YGCoIvYWryf51ZsY0H+NlZt3YMZnDiiH+dMGsznxw+kf05msksUkS5KQdAFbdpVyrMrtrMgfxsbdpaSmmLMGDOAcyYO4nPjB9I7Kz3ZJYpIF6Ig6MLcnXUflvBMftBTKPh4PxmpKcw8Jo9zJw/m9GOPpEdGarLLFJFOTkHQTbg7y7cUsyB/G8+t2M7OknJ6ZqTymeOO5NxJgznlEwPITFMoiMiBFATdUHWN87f3P2JB/jYWrtpO8b5KemelMXv8QM6dNISTR/UjLVVXB4tIQEHQzVVW1/DmhkKeWb6NF9fsoLS8igE5GZw1YRDnTBrM8cNzSUmxZJcpIkmkIIiRsspqXlu/kwX523h57U7Kq2oY0rcHZ08MQmHc4N6YKRRE4kZBEFOl5VX8ec0OFuRvY/Hfd1FV44wakM3ZkwZz7qRBjDmiV7JLFJEOoiAQivdV8MKqD1mQv42/birCHfpnZ5CVnkpmegpZaalkpacEy2nBazClkJmW2CZxe+P2Qdv61/r59FRTT0QkiQ4WBGkdXYwkR9+eGVw4bTgXThvOzj1lPL9yO+/tLKWssoayqmrKK2sor6qmrLKakrKqcL6GsspgXXlVDeVVh/+UtRSjLlxqgyMzLYXM9FT69EhnQE4GeTmZDMjJZECvDPpn18/365mhE98iEVIQxNARvbOYO2PkIb+vpsapqK6pC4YgJBovV1NWVUN54uvB2lfVsHtfBRt3lrKrtJyKJsLGDPr1zGBATib9c4LX2pAI5uvX9c/J0CW0IodIQSCtlpJiZKUEf9VHwd0pKa+isKScwtIKikrLKSwtZ1dpBYWl5eH6cvILiiksKWdvRXWT++mdldZEWDQMkbxwW88M/RcQ0f8C6TTMjN5Z6fTOSmdUXsvt91dUh0ERhETR3oq6sCgsrWBXaTnrPiyhsKSQPWVVTe6jR3pq3aGojNQUHMcdas+cuXvCfLg+PK/m9bN170tsl3j+rXG7pvaPAwbZGWn0ykojJzONXlnpCfNp5ITzvbPSyclKaJcZLKfqMmE5DAoC6bJ6ZKQyrF9PhvXr2WLbiqoaivaWU1hSUR8epeUUhb2NotIKqmuC38RmweEoI/ilmniO2yxYW7vOwnX187VztfsgYT5hfcIyCe0c2FdeRUlZFf/Yu4+SsipKyiopLa+iphXXdfTMSK0Lh5ysdHonhkgYFr0TQqY+WNLCYEmnZ3qq7juJGQWBxEJGWgqD+vTossN6uzv7KqopDUOiNhxKyqooLatiT7hcWhauK69f9+Husrp1peVN94wSmQU9pdQwyYzgsGBt6NW91m5LmK9bHwZdijUMz9r52vcQviel0Xuoaxe0rW2fYkZqSv187XZLmE9Jqd1n4nZIrV2XQsP21sL+DNJTU8hIS6m7wCEzLbyaLq2p9cF8RmoKmenBckZqSqe+ak5BINIFmBnZmWlkZ6ZxZO/D3091jbO3IjEwKtkThkkQLJWUllWxt6Iad6ipPQzmTo3XH9oKeifhYa6wXe2hMq+br31PwyI3BLgAAAbwSURBVHUtvccJ9l/XFqemJmhfVVNDRXUwX9umxuu31+634Xzwc9fWc8B7/cD31m6vrvFW9cRaozYkMtJqr5irD5P6QEmtC4/EbbXhcvzwXKaP7t8+BSVQEIjESGpK/XkYab2q6pq6S6grqoJLrcurauouuz7Y+vqp9jLtxu8Jrq4rKauisKqC8qrqBusTL93+5szRXS8IzGw28AsgFfi1u9/aaHsm8DBwAlAEfMXdN0dZk4jIoUpLTSEtNYXsJD0byj24dDsqkd2lY2apwL3AmcBY4CIzG9uo2T8DH7v7GOAO4Lao6hER6arMLDxUFM2l21HerjkN2ODum9y9ApgPzGnUZg7wUDj/JHC6deYzKiIi3VCUh4aGAFsSlguAk5pr4+5VZrYb6A8UJjYysyuBK8PFUjNbf5g1DWi875jT99GQvo96+i4a6g7fx1HNbegSJ4vdfR4wr637MbMlzQ26FEf6PhrS91FP30VD3f37iPLQ0FZgWMLy0HBdk23MLA3oQ3DSWEREOkiUQfAOcLSZjTSzDOBCYEGjNguAfwrnzwde8a42LraISBcX2aGh8Jj/vwCLCC4ffcDdV5vZLcASd18A/AZ4xMw2AB8RhEWU2nx4qZvR99GQvo96+i4a6tbfR5d7MI2IiLQvPe1DRCTmFAQiIjEXmyAws9lmtt7MNpjZD5JdTzKZ2TAze9XM1pjZajP7drJrSjYzSzWzd83s2WTXkmxm1tfMnjSzdWa21symJ7umZDGz68L/I6vM7HEzy0p2TVGIRRC0criLOKkCvuvuY4GTgW/F/PsA+DawNtlFdBK/AF5w92OBScT0ezGzIcC1wFR3H09w0UvUF7QkRSyCgNYNdxEb7r7d3ZeF8yUE/9GHJLeq5DGzocBZwK+TXUuymVkf4FSCK/pw9wp3L05uVUmVBvQI73PqCWxLcj2RiEsQNDXcRWx/8SUysxHAFODt5FaSVHcC3weiG96x6xgJ7AIeDA+V/drMspNdVDK4+1bgduAfwHZgt7u/mNyqohGXIJAmmFkO8AfgO+6+J9n1JIOZnQ3sdPelya6lk0gDjgd+5e5TgL1ALM+pmVkuwZGDkcBgINvMLkluVdGISxC0ZriLWDGzdIIQeMzdn0p2PUk0AzjXzDYTHDI8zcweTW5JSVUAFLh7bQ/xSYJgiKPPAO+7+y53rwSeAj6Z5JoiEZcgaM1wF7ERDvX9G2Ctu/882fUkk7v/m7sPdfcRBP8uXnH3bvlXX2u4+4fAFjM7Jlx1OrAmiSUl0z+Ak82sZ/h/5nS66YnzLjH6aFs1N9xFkstKphnApcBKM1servt3d38+iTVJ53EN8Fj4R9Mm4PIk15MU7v62mT0JLCO40u5duulQExpiQkQk5uJyaEhERJqhIBARiTkFgYhIzCkIRERiTkEgIhJzCgLp0sys2syWJ0ztdhesmY0ws1WtaHeTme0zsyMS1pV2ZA0ibRGL+wikW9vv7pOTXQRQCHwXuCHZhSQyszR3r0p2HdK5qUcg3ZKZbTazn5rZSjP7m5mNCdePMLNXzGyFmb1sZsPD9Uea2R/NLD+caocSSDWz/wnHpH/RzHo085EPAF8xs36N6mjwF72Zfc/MbgrnXzOzO8xsSTju/4lm9pSZvWdmP0rYTZqZPRa2edLMeobvP8HMXjezpWa2yMwGJez3TjNbQjC8tshBKQikq+vR6NDQVxK27Xb3CcA9BCOMAtwNPOTuE4HHgLvC9XcBr7v7JIKxdWrvPD8auNfdxwHFwJeaqaOUIAwO9RdvhbtPBe4D/gR8CxgPzDWz/mGbY4BfuvtxwB7gm+FYUXcD57v7CeFn/zhhvxnuPtXd//sQ65EY0qEh6eoOdmjo8YTXO8L56cAXw/lHgJ+G86cBlwG4ezWwOxx98n13rx2GYykw4iC13AUsN7PbD6H+2jGvVgKr3X07gJltIhgosRjY4u5vhe0eJXhYygsEgfFSMAwOqQRDJdf63SHUIDGnIJDuzJuZPxTlCfPVQHOHhnD3YjP7X4K/6mtV0bDn3fhRh7X7r2n0WTXU//9sXLsDRhAczT1Gcm9zdYo0pkND0p19JeH1r+H8X6h/3ODFwBvh/MvA1VD3/OI+h/mZPwe+Qf0v8R3AEWbW38wygbMPY5/DE54b/FXgTWA9kFe73szSzWzcYdYsMacgkK6u8TmCWxO25ZrZCoLj9teF664BLg/XX0r9Mf1vA7PMbCXBIaDDeoazuxcCfwQyw+VK4Bbgb8BLwLrD2O16gudKrwVyCR4aUwGcD9xmZvnAcrrpWPkSPY0+Kt1S+KCZqeEvZhE5CPUIRERiTj0CEZGYU49ARCTmFAQiIjGnIBARiTkFgYhIzCkIRERi7v8D1e8g7vym+5oAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Analyze the accuracy curve\n",
        "\n",
        "plt.plot(history[:,2:4])\n",
        "plt.legend(['Tr Accuracy', 'Val Accuracy'])\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0,1)\n",
        "# plt.savefig('cifar10_accuracy_curve.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "RRD3qCNWin8y",
        "outputId": "ad3178e2-e3a4-4b9d-e12a-a8c17bb30f9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data in train folder of the fruit_dataset is checked and any unrelated images to the label will be moved to appropriate folder"
      ],
      "metadata": {
        "id": "U_sw7raG_PfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is loaded again."
      ],
      "metadata": {
        "id": "KWZfkX7d_9bU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "####################################################\n",
        "#       Create Train, Valid and Test sets\n",
        "####################################################\n",
        "train_data_path = dataset + '/train' \n",
        "test_data_path = dataset + '/validation'\n",
        "\n",
        "train_image_paths = [] #to store image paths in list\n",
        "classes = [] #to store class values\n",
        "\n",
        "#1.\n",
        "# get all the paths from train_data_path and append image paths and class to to respective lists\n",
        "\n",
        "for data_path in glob.glob(train_data_path + '/*'):\n",
        "    classes.append(data_path.split('/')[-1]) \n",
        "    train_image_paths.append(glob.glob(data_path + '/*'))\n",
        "    \n",
        "train_image_paths = list(flatten(train_image_paths))\n",
        "random.shuffle(train_image_paths)\n",
        "\n",
        "print('train_image_path example: ', train_image_paths[0])\n",
        "print('class example: ', classes[0])\n",
        "\n",
        "#2.\n",
        "# create the test_image_paths\n",
        "test_image_paths = []\n",
        "for data_path in glob.glob(test_data_path + '/*'):\n",
        "    test_image_paths.append(glob.glob(data_path + '/*'))\n",
        "\n",
        "test_image_paths = list(flatten(test_image_paths))\n",
        "\n",
        "print(\"Train size: {}\\nTest size: {}\".format(len(train_image_paths), len(test_image_paths)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87671817-28d9-4191-e155-da64b2dcbb62",
        "id": "beO9A-qr_5Z-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_image_path example:  /content/gdrive/My Drive/fruit_dataset/train/tomato/9223fc18a7824a1c00eeb2071f0035e300d2e51d.jpg\n",
            "class example:  watermelon\n",
            "Train size: 521\n",
            "Test size: 323\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "#      Create dictionary for class indexes\n",
        "#######################################################\n",
        "\n",
        "idx_to_class = {i:j for i, j in enumerate(classes)}\n",
        "class_to_idx = {value:key for key,value in idx_to_class.items()}\n",
        "class_to_idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a173cca-2364-486c-933f-fa6d8c59f2d8",
        "id": "L5qaBfTT_5Z_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'watermelon': 0, 'tomato': 1, 'durian': 2, 'pumpkin': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "#######################################################\n",
        "#               Define Dataset Class\n",
        "#######################################################\n",
        "\n",
        "class fruitDataset(Dataset):\n",
        "    def __init__(self, image_paths, transform=False):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_filepath = self.image_paths[idx]\n",
        "#         image = cv2.imread(image_filepath)\n",
        "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = Image.open(image_filepath) # if using torchvision transforms\n",
        "        # print(image_filepath)\n",
        "        label = image_filepath.split('/')[-2]\n",
        "        # print(label)\n",
        "        label = class_to_idx[label]\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image) # if using torchvision transforms\n",
        "        \n",
        "        return image, label"
      ],
      "metadata": {
        "id": "tP_Gxnl4_5Z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "#                  Create Dataset\n",
        "#######################################################\n",
        "\n",
        "train_dataset = fruitDataset(train_image_paths,image_transforms['train'])\n",
        "test_dataset = fruitDataset(test_image_paths,image_transforms['test'])"
      ],
      "metadata": {
        "id": "sDs1MNr5_5Z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.transform"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e80ea1b3-43df-4e9f-9e73-3886c0eb45c5",
        "id": "hEZkSjti_5Z_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Compose(\n",
              "    RandomResizedCrop(size=(256, 256), scale=(0.8, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
              "    RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)\n",
              "    RandomHorizontalFlip(p=0.5)\n",
              "    CenterCrop(size=(224, 224))\n",
              "    ToTensor()\n",
              "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "eqQqawD2_5Z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "#                  Create Dataloader                     #\n",
        "#######################################################\n",
        "\n",
        "# Turn train and test custom Dataset's into DataLoader's\n",
        "from torch.utils.data import DataLoader\n",
        "trainloader = DataLoader(dataset=train_dataset, # use custom created train Dataset\n",
        "                                     batch_size=32, # how many samples per batch?\n",
        "                                     num_workers=0, # how many subprocesses to use for data loading? (higher = more)\n",
        "                                     shuffle=True) # shuffle the data?\n",
        "\n",
        "testloader = DataLoader(dataset=test_dataset, # use custom created test Dataset\n",
        "                                    batch_size=32, \n",
        "                                    num_workers=0, \n",
        "                                    shuffle=False) # don't usually need to shuffle testing data\n",
        "\n",
        "train_data_size = len(trainloader.dataset)\n",
        "test_data_size = len(testloader.dataset)\n",
        "\n",
        "print(train_data_size)\n",
        "print(test_data_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd538fc0-a0dc-4b9d-e967-85b409cc0235",
        "id": "-m6CiHOu_5Z_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "521\n",
            "323\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#######################\n",
        "# MODEL IS DEFINED\n",
        "\n",
        "model = models.resnet18(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "model.fc = nn.Linear(num_ftrs, 4)\n",
        "\n",
        "# 2. LOSS AND OPTIMIZER\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# 3. move the model to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c972b344-63bd-4e17-dd04-986b712723a5",
        "id": "rrMPMVog_5aA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "\n",
        "model.to(device)\n",
        "summary(model, (3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5705f646-c335-41b4-d387-d4c359bc87dd",
        "id": "IJZgfH65_5aA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
            "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
            "             ReLU-14           [-1, 64, 56, 56]               0\n",
            "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
            "             ReLU-17           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
            "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
            "             ReLU-21          [-1, 128, 28, 28]               0\n",
            "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
            "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
            "             ReLU-26          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
            "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
            "             ReLU-30          [-1, 128, 28, 28]               0\n",
            "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
            "             ReLU-33          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
            "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
            "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
            "             ReLU-37          [-1, 256, 14, 14]               0\n",
            "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
            "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
            "             ReLU-42          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
            "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
            "             ReLU-46          [-1, 256, 14, 14]               0\n",
            "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
            "             ReLU-49          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
            "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
            "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-53            [-1, 512, 7, 7]               0\n",
            "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
            "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
            "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-58            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
            "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-62            [-1, 512, 7, 7]               0\n",
            "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-65            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
            "           Linear-68                    [-1, 4]           2,052\n",
            "================================================================\n",
            "Total params: 11,178,564\n",
            "Trainable params: 11,178,564\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 62.79\n",
            "Params size (MB): 42.64\n",
            "Estimated Total Size (MB): 106.00\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import time # to calculate training time\n",
        "\n",
        "def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n",
        "    '''\n",
        "    Function to train and validate\n",
        "    Parameters\n",
        "        :param model: Model to train and validate\n",
        "        :param loss_criterion: Loss Criterion to minimize\n",
        "        :param optimizer: Optimizer for computing gradients\n",
        "        :param epochs: Number of epochs (default=25)\n",
        "  \n",
        "    Returns\n",
        "        model: Trained Model with best validation accuracy\n",
        "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
        "    '''\n",
        "    \n",
        "    start = time.time()\n",
        "    history = []\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
        "        \n",
        "        # Set to training mode\n",
        "        model.train()\n",
        "        \n",
        "        # Loss and Accuracy within the epoch\n",
        "        train_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "        \n",
        "        valid_loss = 0.0\n",
        "        valid_acc = 0.0\n",
        "        \n",
        "        for i, (inputs, labels) in enumerate(trainloader):\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Clean existing gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass - compute outputs on input data using the model\n",
        "            outputs = model(inputs)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = loss_criterion(outputs, labels)\n",
        "            \n",
        "            # Backpropagate the gradients\n",
        "            loss.backward()\n",
        "            \n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Compute the total loss for the batch and add it to train_loss\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            \n",
        "            # Compute the accuracy\n",
        "            ret, predictions = torch.max(outputs.data, 1)\n",
        "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "            \n",
        "            # Convert correct_counts to float and then compute the mean\n",
        "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "            \n",
        "            # Compute total accuracy in the whole batch and add to train_acc\n",
        "            train_acc += acc.item() * inputs.size(0)\n",
        "            \n",
        "            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
        "\n",
        "            \n",
        "        # Validation - No gradient tracking needed\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Set to evaluation mode\n",
        "            model.eval()\n",
        "\n",
        "            # Validation loop\n",
        "            for j, (inputs, labels) in enumerate(testloader):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Forward pass - compute outputs on input data using the model\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = loss_criterion(outputs, labels)\n",
        "\n",
        "                # Compute the total loss for the batch and add it to valid_loss\n",
        "                valid_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                # Calculate validation accuracy\n",
        "                ret, predictions = torch.max(outputs.data, 1)\n",
        "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "\n",
        "                # Convert correct_counts to float and then compute the mean\n",
        "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "                # Compute total accuracy in the whole batch and add to valid_acc\n",
        "                valid_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
        "            \n",
        "        # Find average training loss and training accuracy\n",
        "        avg_train_loss = train_loss/train_data_size \n",
        "        avg_train_acc = train_acc/train_data_size\n",
        "\n",
        "        # Find average training loss and training accuracy\n",
        "        avg_test_loss = valid_loss/test_data_size \n",
        "        avg_test_acc = valid_acc/test_data_size\n",
        "\n",
        "        history.append([avg_train_loss, avg_test_loss, avg_train_acc, avg_test_acc])\n",
        "                \n",
        "        epoch_end = time.time()\n",
        "    \n",
        "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_test_loss, avg_test_acc*100, epoch_end-epoch_start))\n",
        "        \n",
        "        # Save if the model has best accuracy till now\n",
        "        torch.save(model, 'cifar10_model_'+str(epoch)+'.pt')\n",
        "            \n",
        "    return model, history"
      ],
      "metadata": {
        "id": "5wGdRG8F_5aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 4. Train the model for 10 epochs\n",
        " \n",
        "num_epochs = 10\n",
        "trained_model, history = train_and_validate(model, criterion, optimizer, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce30a17f-6fb3-4a25-cbf4-1cbdae42e614",
        "id": "FlJ_LkVe_5aB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\n",
            "Epoch : 000, Training: Loss: 1.0139, Accuracy: 54.1267%, \n",
            "\t\tValidation : Loss : 1.6903, Accuracy: 46.4396%, Time: 6.6512s\n",
            "Epoch: 2/10\n",
            "Epoch : 001, Training: Loss: 0.4952, Accuracy: 86.9482%, \n",
            "\t\tValidation : Loss : 1.3373, Accuracy: 80.8050%, Time: 5.7356s\n",
            "Epoch: 3/10\n",
            "Epoch : 002, Training: Loss: 0.3687, Accuracy: 89.6353%, \n",
            "\t\tValidation : Loss : 1.2084, Accuracy: 82.6625%, Time: 5.6629s\n",
            "Epoch: 4/10\n",
            "Epoch : 003, Training: Loss: 0.3201, Accuracy: 90.7869%, \n",
            "\t\tValidation : Loss : 1.1212, Accuracy: 82.9721%, Time: 5.6441s\n",
            "Epoch: 5/10\n",
            "Epoch : 004, Training: Loss: 0.2702, Accuracy: 92.7063%, \n",
            "\t\tValidation : Loss : 1.0012, Accuracy: 84.2105%, Time: 5.6608s\n",
            "Epoch: 6/10\n",
            "Epoch : 005, Training: Loss: 0.2715, Accuracy: 90.7869%, \n",
            "\t\tValidation : Loss : 0.9502, Accuracy: 84.8297%, Time: 5.9284s\n",
            "Epoch: 7/10\n",
            "Epoch : 006, Training: Loss: 0.2318, Accuracy: 92.5144%, \n",
            "\t\tValidation : Loss : 0.8761, Accuracy: 85.1393%, Time: 5.7190s\n",
            "Epoch: 8/10\n",
            "Epoch : 007, Training: Loss: 0.2276, Accuracy: 92.3225%, \n",
            "\t\tValidation : Loss : 0.8657, Accuracy: 84.8297%, Time: 6.1871s\n",
            "Epoch: 9/10\n",
            "Epoch : 008, Training: Loss: 0.2140, Accuracy: 92.8983%, \n",
            "\t\tValidation : Loss : 0.7942, Accuracy: 84.5201%, Time: 5.7043s\n",
            "Epoch: 10/10\n",
            "Epoch : 009, Training: Loss: 0.2092, Accuracy: 92.3225%, \n",
            "\t\tValidation : Loss : 0.7562, Accuracy: 84.2105%, Time: 5.6776s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Analyze the loss curve\n",
        "\n",
        "history = np.array(history)\n",
        "plt.plot(history[:,0:2])\n",
        "plt.legend(['Tr Loss', 'Val Loss'])\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(0,3)\n",
        "# plt.savefig('cifar10_loss_curve.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "_GWvA90TBWBh",
        "outputId": "a2e13a72-a447-4c35-fa38-9a32d736b66a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history[:,2:4])\n",
        "plt.legend(['Tr Accuracy', 'Val Accuracy'])\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0,1)\n",
        "# plt.savefig('cifar10_accuracy_curve.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "GG_I9bxsBaOv",
        "outputId": "c8eaf0c5-8759-4e57-cf45-122076e3b1cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the model:\n",
        "\n",
        "\n",
        "model_save_name = 'fruit_classifier.pt'\n",
        "path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "metadata": {
        "id": "OMU88SuW5TbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test on webcam\n",
        "\n",
        "import torchvision.models as models\n",
        "import torch\n",
        "import time\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "awPNM3PUjKMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess():\n",
        "    \"\"\"\n",
        "    Define the transform for the input image/frames.\n",
        "    Resize, crop, convert to tensor, and apply ImageNet normalization stats.\n",
        "    \"\"\"\n",
        "    transform =  transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
        "                             std=[0.229, 0.224, 0.225]),])\n",
        "    return transform\n",
        "    "
      ],
      "metadata": {
        "id": "j_J7Ne4N0jJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the model\n",
        "\n",
        "# model_save_name = 'fruit_classifier.pt'\n",
        "# path = F\"/content/gdrive/My Drive/{model_save_name}\"\n",
        "# model.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "x6DEESoK1z_h",
        "outputId": "10185473-03f4-40f2-b817-e30a44173e67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-6357e1e2748a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_save_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'fruit_classifier.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mF\"/content/gdrive/My Drive/{model_save_name}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: '_IncompatibleKeys' object has no attribute 'load_state_dict'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the computation device.\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Load the model.\n",
        "path = F\"/content/gdrive/My Drive/{model_save_name}\"\n",
        "model.load_state_dict(torch.load(path)) \n",
        "model.eval()\n",
        "# Load the ImageNet class names.\n",
        "categories = ['watermelon', 'tomato', 'durian', 'pumpkin']\n",
        "# Initialize the image transforms.\n",
        "transform = preprocess()\n",
        "# print(f\"Computation device: {device}\")\n",
        "# # image = cv2.imread(args['input'])\n",
        "# image = cv2.imread(\"dog.jpg\")\n",
        "# rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "# # Apply transforms to the input image.\n",
        "# input_tensor = transform(image)\n",
        "# # Add the batch dimension.\n",
        "# input_batch = input_tensor.unsqueeze(0)\n",
        "# # Move the input tensor and model to the computation device.\n",
        "# input_batch = input_batch.to(device)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFaTv2eQ09rI",
        "outputId": "597897ba-6ba7-4d34-c81a-fca8595de074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cap = cv2.VideoCapture(0)\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read() \n",
        "    \n",
        "#     rgb_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    # Apply transforms to the input image.\n",
        "    input_tensor = transform(frame)\n",
        "    # Add the batch dimension.\n",
        "    input_batch = input_tensor.unsqueeze(0)\n",
        "    input_batch = input_batch.to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        start_time = time.time()\n",
        "        output = model(input_batch)\n",
        "        end_time = time.time()\n",
        "    # Get the softmax probabilities.\n",
        "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "    # Check the top 5 categories that are predicted.\n",
        "    top5_prob, top5_catid = torch.topk(probabilities, 1)\n",
        "    for i in range(top5_prob.size(0)):\n",
        "        cv2.putText(frame, f\"{top5_prob[i].item()*100:.3f}%\", (15, (i+1)*30), \n",
        "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    1, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "        cv2.putText(frame, f\"{categories[top5_catid[i]]}\", (160, (i+1)*30), \n",
        "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    1, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "        print(categories[top5_catid[i]], top5_prob[i].item())\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    cv2.imshow('Raw Webcam Feed', frame)\n",
        "\n",
        "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
        "        cap.release()\n",
        "        break\n",
        "\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "Ch2UNskA1IOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import dependencies\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "\n",
        "\n",
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ],
      "metadata": {
        "id": "fE15V6MO1Ova"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "metadata": {
        "id": "gXlazOeX1X5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0 \n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # convert JS response to OpenCV Image\n",
        "    frame = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    #     rgb_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    # Apply transforms to the input image.\n",
        "    input_tensor = transform(frame)\n",
        "    # Add the batch dimension.\n",
        "    input_batch = input_tensor.unsqueeze(0)\n",
        "    input_batch = input_batch.to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        start_time = time.time()\n",
        "        output = model(input_batch)\n",
        "        end_time = time.time()\n",
        "    # Get the softmax probabilities.\n",
        "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "    # Check the top 5 categories that are predicted.\n",
        "    top5_prob, top5_catid = torch.topk(probabilities, 1)\n",
        "    \n",
        "    cv2.putText(frame, f\"{top5_prob[0].item()*100:.3f}%\", (15, (1)*30), \n",
        "                cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                1, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "    cv2.putText(frame, f\"{categories[top5_catid[0]]}\", (160, (1)*30), \n",
        "                cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                1, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "    print(categories[top5_catid[0]], top5_prob[0].item())\n",
        "    cv2_imshow(frame)"
      ],
      "metadata": {
        "id": "zk_DCjxP1hsH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "1d01136c-0d61-4563-eed1-79e8716db437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9f1083880451>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatches\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# start streaming video from webcam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvideo_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# label for video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlabel_html\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Capturing...'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'video_stream' is not defined"
          ]
        }
      ]
    }
  ]
}